{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtOboKdvjZa+JVbdrsBXKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prrmzz/FakeNewsDetection/blob/main/FakeNewsDetection_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F_0Mq5JzJZo",
        "outputId": "52c341c0-8dc7-41ab-809e-4d1b319c8192"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Conv1D, MaxPool1D, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "MNzWm71h1ZiY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(base_path):\n",
        "    \"\"\"\n",
        "    Load train, test, and validation datasets from separate TSV files.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    for split in [\"train\", \"test\", \"valid\"]:\n",
        "        file_path = os.path.join(base_path, f\"{split}.tsv\")\n",
        "        datasets[split] = pd.read_csv(file_path, sep='\\t', header=None, names=[\n",
        "            \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state_info\",\n",
        "            \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\",\n",
        "            \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
        "        ])\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "XxN-CxZT1caf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(data, column):\n",
        "\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    translator = str.maketrans('', '', punctuation)\n",
        "\n",
        "    def clean_text(text):\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [w.translate(translator) for w in tokens]\n",
        "        tokens = [w for w in tokens if w not in stop_words and w.isalpha()]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "    return data"
      ],
      "metadata": {
        "id": "obr3TJa61emo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_labels(data):\n",
        "\n",
        "    data['binary_label'] = data['label'].apply(lambda x: 1 if x in ['half-true', 'mostly-true', 'true'] else 0)\n",
        "    return data"
      ],
      "metadata": {
        "id": "dohBC3yQ1h2Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences(texts, tokenizer=None, max_len=None):\n",
        "\n",
        "    if not tokenizer:\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    if not max_len:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "    return padded_sequences, tokenizer, max_len"
      ],
      "metadata": {
        "id": "S1a7H_Id1kZZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_model(input_length, vocab_size):\n",
        "\n",
        "    input_layer = Input(shape=(input_length,))\n",
        "    embedding_layer = Embedding(vocab_size, 128)(input_layer)\n",
        "    conv_layer = Conv1D(filters=64, kernel_size=4, activation='relu')(embedding_layer)\n",
        "    dropout_layer = Dropout(0.5)(conv_layer)\n",
        "    pooling_layer = MaxPool1D(pool_size=2)(dropout_layer)\n",
        "    flatten_layer = Flatten()(pooling_layer)\n",
        "    dense_layer = Dense(64, activation='relu')(flatten_layer)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "NEbrQf891odx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(datasets, base_path):\n",
        "\n",
        "    train_data, test_data, valid_data = datasets['train'], datasets['test'], datasets['valid']\n",
        "\n",
        "    train_data = preprocess_text(train_data, 'statement')\n",
        "    train_data = simplify_labels(train_data)\n",
        "\n",
        "    test_data = preprocess_text(test_data, 'statement')\n",
        "    test_data = simplify_labels(test_data)\n",
        "\n",
        "    valid_data = preprocess_text(valid_data, 'statement')\n",
        "    valid_data = simplify_labels(valid_data)\n",
        "\n",
        "    X_train_padded, tokenizer, max_len = prepare_sequences(train_data['statement'])\n",
        "    X_test_padded, _, _ = prepare_sequences(test_data['statement'], tokenizer, max_len)\n",
        "    X_valid_padded, _, _ = prepare_sequences(valid_data['statement'], tokenizer, max_len)\n",
        "\n",
        "    y_train = train_data['binary_label']\n",
        "    y_test = test_data['binary_label']\n",
        "    y_valid = valid_data['binary_label']\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    model = build_cnn_model(max_len, vocab_size)\n",
        "    model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_valid_padded, y_valid))\n",
        "\n",
        "    model.save(os.path.join(base_path, 'liar_cnn_model.h5'))\n",
        "    with open(os.path.join(base_path, 'tokenizer.pkl'), 'wb') as f:\n",
        "        import pickle\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Fake\", \"True\"]))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "avGLzoUc1sPI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def main():\n",
        "\n",
        "    base_path = '/content/drive/MyDrive/LIAR.ds'\n",
        "\n",
        "    print(\"Starting the process...\")\n",
        "\n",
        "    try:\n",
        "        print(\"Loading datasets...\")\n",
        "        datasets = load_datasets(base_path)\n",
        "\n",
        "        for split, data in datasets.items():\n",
        "            print(f\"Loaded {split} dataset with {len(data)} records.\")\n",
        "\n",
        "        print(\"Starting training and evaluation...\")\n",
        "        train_and_evaluate(datasets, base_path)\n",
        "\n",
        "        print(\"Process completed successfully!\")\n",
        "\n",
        "    except FileNotFoundError as fnfe:\n",
        "        print(f\"FileNotFoundError: {fnfe}. Please verify that the path and files are correct.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dux_lu5G1vnA",
        "outputId": "6f8cd1e8-dc6d-41f5-d7a7-6dacfa94b52f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the process...\n",
            "Loading datasets...\n",
            "Loaded train dataset with 10240 records.\n",
            "Loaded test dataset with 1267 records.\n",
            "Loaded valid dataset with 1284 records.\n",
            "Starting training and evaluation...\n",
            "Epoch 1/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 88ms/step - accuracy: 0.5628 - loss: 0.6823 - val_accuracy: 0.5857 - val_loss: 0.6708\n",
            "Epoch 2/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 91ms/step - accuracy: 0.7282 - loss: 0.5598 - val_accuracy: 0.5717 - val_loss: 0.7245\n",
            "Epoch 3/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 86ms/step - accuracy: 0.8997 - loss: 0.2569 - val_accuracy: 0.5740 - val_loss: 0.8885\n",
            "Epoch 4/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 90ms/step - accuracy: 0.9671 - loss: 0.1002 - val_accuracy: 0.5802 - val_loss: 1.3207\n",
            "Epoch 5/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.9823 - loss: 0.0492 - val_accuracy: 0.5771 - val_loss: 1.6679\n",
            "Epoch 6/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.9894 - loss: 0.0327 - val_accuracy: 0.5701 - val_loss: 1.9622\n",
            "Epoch 7/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 84ms/step - accuracy: 0.9915 - loss: 0.0241 - val_accuracy: 0.5646 - val_loss: 2.2915\n",
            "Epoch 8/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 91ms/step - accuracy: 0.9937 - loss: 0.0205 - val_accuracy: 0.5592 - val_loss: 2.5853\n",
            "Epoch 9/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.9920 - loss: 0.0236 - val_accuracy: 0.5358 - val_loss: 2.5682\n",
            "Epoch 10/10\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - accuracy: 0.9937 - loss: 0.0177 - val_accuracy: 0.5553 - val_loss: 2.7182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.7315\n",
            "Test Accuracy: 0.5588\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.49      0.46      0.48       553\n",
            "        True       0.60      0.63      0.62       714\n",
            "\n",
            "    accuracy                           0.56      1267\n",
            "   macro avg       0.55      0.55      0.55      1267\n",
            "weighted avg       0.56      0.56      0.56      1267\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[256 297]\n",
            " [262 452]]\n",
            "Process completed successfully!\n"
          ]
        }
      ]
    }
  ]
}